{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习\n",
    "\n",
    "在本章中\n",
    "* 首先，解释强化学习是什么以及它擅长什么\n",
    "* 然后，介绍两个在深度强化学习领域最重要的技术\n",
    "    * 策略梯度\n",
    "    * 深度Q网络(DQN)\n",
    "* 包括讨论马尔可夫决策过程(MDP)\n",
    "\n",
    "我们将使用这些技术来训练一个模型来平衡移动车上的杆子，另一个玩Atari游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 学习优化奖励(Learning to optimaze rewards)\n",
    "\n",
    "在强化学习中，智能体在环境(environment)中观察(observation)并且做出决策(action)，随后它会得到奖励(reward)。\n",
    "\n",
    "目标时区学习如何行动能最大化期望奖励。\n",
    "\n",
    "简单来说，智能体在环境中行动，并且在实验和错误中去学习最大化它的愉快，最小化它的痛苦。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 策略搜索(Policy Search)\n",
    "\n",
    "被智能体使用去改变它的行为的算法叫做策略。例如：**策略可以是一个把观测当输入，行为当做输出的神经网络**。\n",
    "\n",
    "![title](policy.png)\n",
    "\n",
    "**随机策略**：例如一个真空吸尘器，它的奖励是在30分钟内捡起的灰尘的数量，它的策略可以是每秒以概率$P$向前移动，或者以概率$1-P$随机地向左或向右旋转，旋转的角度将是$-R$和$+R$之间的随机角度。由于该策略涉及一些随机性，所以称为随机策略。\n",
    "\n",
    "**训练方法**\n",
    "* 通过调整两个策略参数：概率$P$和角度范围$R$。这些参数尝试不同的值，并选择执行最佳的组合，但是当策略空间太大，找到一组好的参数如同大海捞针\n",
    "* 另一种搜索策略空间的方式是遗传算法：随机创造一个包含100个策略的第一代基因，随后杀死80个糟糕的策略，随后让20个幸存策略繁衍4代。一个后代只是它父辈基因的复制品加上一些随机变异。幸存的策略加上他们的后代共同构成了第二代。继续以这种方式迭代，直到找到一个好的策略。\n",
    "* 另一种方法是使用优化方法：通过评估奖励关于策略参数的梯度，然后通过跟随梯度向更高的奖励(梯度上升)调整这些参数，这种方法被称为策略梯度(Policy Gradient,PG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. OpenAI的介绍\n",
    "\n",
    "强化学习的一个挑战是，为了训练智能体，首先需要有一个**工作环境**。\n",
    "\n",
    "训练在现实世界是困难和缓慢的，所以通常需要一个模拟环境，至少需要引导训练。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03321464, -0.03889074,  0.03261536, -0.02290879])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用$make()$函数创建一个环境，在此例中是CartPole环境。这是一个2D环境，其中推车可以被左右加速，以平衡放置在它上面的平衡杆。\n",
    "* 使用$reset()$函数初始化，返回第一个观察的结果。观察取决于环境的类型。对于CartPole环境，每个观测是包含四个浮点数的1D Numpy向量，这些浮点数分别代表：\n",
    "    * 推车的水平位置（0为中心）\n",
    "    * 推车的速度\n",
    "    * 杆的角度（0维垂直）\n",
    "    * 杆的角速度\n",
    "* 使用$render()$函数显示如图所示的环境\n",
    "\n",
    "![title](cartpole_env.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用$render()$让图像以一个Numpy数组格式返回，可以将$mode$参数设置为$reg_array$（注意，其他环境可能支持不同的模式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render(mode='rgb_array')\n",
    "img.shape  # height,width,channels(3=RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole（和其他一些环境）还是会将图像呈现到屏幕上。避免这种情况的唯一方式是使用一个fake X服务器，如XVFB或XDimMy。\n",
    "\n",
    "可以使用以下命令安装XVFB和启动：\n",
    "\n",
    "Python :xvfb-run -s \"screen 0 1400x900x24\" python\n",
    "\n",
    "或者使用xvfbwrapper包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查询环境的动作空间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete(2)表示可能的动作是整数0和1,表示向左（0）或向右（1）的加速。（其他环境可能有更多的动作甚至是连续的）\n",
    "\n",
    "因为杆子向右倾斜，让我们向右加速推车："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action = 1  # acclerate right\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03399245,  0.15574866,  0.03215719, -0.3051254 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$step()$表示执行给定的动作并返回四个值：\n",
    "* $obs$：这是新的观测，小车现在正在往右走（obs[1]>0,注意：当前速度为正，向右为正）。平衡杆仍然向右倾斜(obs[2]>0)，但是它的角速度现在为负(obs[3]<0)，所以它在下一步可能向左倾斜。\n",
    "\n",
    "* $reward$：在这个环境中，无论做什么，每一步都会得到1.0的奖励，所以游戏的目标就是尽可能长的运行\n",
    "\n",
    "* $done$：当游戏结束时这个值为True\n",
    "\n",
    "* $info$：该字典可以在其他环境中提供额外的调试信息。这些数据不应该用于训练（这是作弊）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "硬编码一个简单测策略，当杆向左倾斜时加速左边，当杆向右倾斜时加速右边。使用这个策略来获得超过500步的平均回报："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle<0 else 1\n",
    "\n",
    "totals = []  # 获取所有episode的总奖励\n",
    "\n",
    "for episode in range(500):\n",
    "    episode_reward = 0  # 获取每个episode的总奖励\n",
    "    obs = env.reset()\n",
    "    for step in range(1000):  # 最多1000步，我们不想让它永远运行下去\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        env.render()\n",
    "    totals.append(episode_reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.02, 8.666925637156465, 24.0, 68.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals),np.std(totals),np.min(totals),np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，即使有500次尝试，这一策略从未使平衡杆在超过68个步骤里保持直立。\n",
    "\n",
    "并且，推车越来越强烈地左右摆动，知道平衡杆倾斜太多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 神经网络策略(Neural Network Policies)\n",
    "\n",
    "让我们创建一个神经网络策略。这个神经网络将把观察作为输入，输出要执行的动作，即估计每个动作的概率，然后我们根据估计的概率随机地选择一个动作。\n",
    "\n",
    "在CartPole环境中，只有两种可能的动作（左或右），所以我们只需要输出一个神经元。他将输出动作0（左）的概率$P$，动作1（右）的概率显然是$1-P$。\n",
    "\n",
    "例如：如果输出0.7，我们将以70%的概率选择动作0,30%的概率选择动作1.\n",
    "![title](neural_policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据神经网络给出的概率来选择随机的动作，而不是选择最高分数额达动作。\n",
    "\n",
    "这种方法使智能体在探索新的行为和利用那些已知可行的行动之间找到正确的平衡。\n",
    "\n",
    "还要注意，在这个特定的环境中过去的动作和观察可以被安全地忽略，因为每个观察都包含环境的完整状态。\n",
    "* 如果有一些隐藏状态，那么也需要过去的行为和观察\n",
    "* 另一个例子是当观测是有噪声的，此时需要用过去的观察来估计最可能的当前状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:universe]",
   "language": "python",
   "name": "conda-env-universe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
